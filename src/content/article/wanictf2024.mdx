---
title: 'WaniCTF 2024開催記'
description: ''
tags: ['WaniCTF', 'AWS']
isPublished: false
publishedAt: '2024-10-27T19:00:00+09:00'
modifiedAt: '2024-10-27T19:00:00+09:00'
thumbnail: '🐊'
---

# WaniCTF 2024開催記

私の所属している大阪大学CTFサークルWani Hackaseは、6月にCTF大会「WaniCTF 2024」を開催しました。WaniCTFは2020年から実施しており、これが5回目の開催となります。

https://wanictf.org/

https://github.com/wani-hackase/wanictf2024-writeup/

この記事ではWaniCTF 2024を支えたインフラを紹介しながら大会を振り返ります。

## スコアサーバー

### リクエストの流れ

CTFにおけるスコアサーバーは、参加者による参加登録やフラグの提出を受け付け、順位表を提供するWebサーバーです。ユーザーからのリクエストは以下の図に示すように処理されます。

{/* ![AWS上で構築されたWebサービスのアーキテクチャ。ユーザーからのアクセスは、Amazon Route 53とAmazon CloudFrontを経由して単一リージョン内のAmazon S3やApplication Load Balancerに到達する。リージョンには1つのVPCが存在する。VPC内には2つのアベイラビリティゾーンがあり、各ゾーンにパブリックサブネットとプライベートサブネットがある。パブリックサブネットにはApplication Load BalancerとNATゲートウェイが配置されている。プライベートサブネットにはAmazon ECSタスク、Amazon RDS (MySQL)、Amazon ElastiCache (Redis)が配置されている。Application Load BalancerはECSタスクにトラフィックを分散する。Amazon ECSタスクはAmazon ECSクラスタ内で動作しており、AWS Fargateを利用している。片方のアベイラビリティゾーンに配置されたAmazon RDSとAmazon ElastiCacheはPrimaryで、もう片方はSecondaryである。Amazon ECSタスクはPrimaryのAmazon RDSとAmazon ElastiCacheにアクセスする。](./wanictf2024/architecture-score-traffic.webp 'w=large') */}

スコアサーバーはSPAとして作られています。静的ファイルはAmazon S3にホストされており、APIリクエストはALBを経由してAmazon ECS上のAPIサーバーに送られます。

各コンポーネントはAZ障害に耐えられるように冗長化しています。ECSタスクは2つのAZに分散して配置され、RDSとElastiCacheはマルチAZ構成を採用しています。ただし、ALBは本来3つのAZに配置しないとAZ障害時に正しく動作しないようです。大会終了後に気づきました。

また、48時間の大会本番以外の期間は、コスト削減のため冗長性を落としています。NATゲートウェイの代わりにNATインスタンスを利用し、ECSのタスク数を削減し、マルチAZ構成を解除しました。

### 運用とデプロイ

APIサーバーのデプロイと運用の流れは以下の通りです。

{/* ![AWS上で動作するアプリケーションのデプロイと運用の流れを示す図。GitHub Actionsで実行されるCI/CDパイプラインは、GitHubのOIDCプロバイダーからIDトークンを受け取りAWSにアクセスする。パイプラインはコンテナイメージをAmazon ECRレジストリにプッシュし、静的アセットをAmazon S3に保存し、キャッシュ無効化リクエストをAmazon CloudFrontに送信する。AWS内にはAmazon ECSタスクが存在する。ECSタスクではアプリケーションコンテナ、OpenTelemetry Collectorコンテナ、およびFluent Bitコンテナが実行されている。アプリケーションコンテナはECRレジストリ上のイメージから作成される。OpenTelemetry CollectorコンテナとFluent Bitコンテナは、それぞれメトリクスとログをGrafana Cloud上のPrometheusとGrafana Lokiに送信する。](./wanictf2024/architecture-score-ops.webp 'w=large') */}

アプリケーションのデプロイ時にはGitHub ActionsからAWSの各サービスにアクセスしています。GitHubのOIDCプロバイダを利用し、安全にIAMロールをAssumeRoleできるようにしています。

メトリクスとログはGrafana Cloudに集約しています。Grafana Cloudの無料プランには保存期間やアクティブユーザー数の制限がありますが、インフラは私一人で管理しており、大会が実際に開催されるのは2日間だけだったため問題なく利用できました。

### 実際のトラフィック

大会期間中に記録したメトリクスをいくつか紹介します。

#### リクエスト数

{/* ![時間帯ごとのリクエスト数を示す折れ線グラフ。期間は6月21日から6月25日まで。](./wanictf2024/metrics-cf-requests.webp 'w=large') */}

こちらのグラフはCloudFrontに対するリクエスト数を示しています。縦軸は5分間のリクエスト数です。一瞬のスパイクを除いたピーク時のリクエスト数は3000 req/min程度でした。

48時間の開催期間とその前後を含む96時間の合計リクエスト数は607万でした。このうち95%がAPIリクエストで、残りの5%が静的ファイルの要求です。現在使用している問題サーバーでは、順位表等のリアルタイム更新のため特定のAPIをブラウザから定期的に呼び出しています。ユーザー数に対してAPIリクエスト数が多いのはこのためです。

### 転送量

{/* ![時間帯ごとの上りと下りの転送量を示す折れ線グラフ。期間は6月21日から6月25日まで。](./wanictf2024/metrics-cf-transfer.webp 'w=large') */}

こちらのグラフはCloudFrontの転送量を示しています。青の線が上り、オレンジの線が下りで、縦軸は5分間の転送量（bytes）です。上りの転送量は下りと比較するとほぼ0です。ピーク時の下り転送量は3Gbps程度でした。

48時間の開催期間とその前後を含む96時間の合計転送量は上り29MB、下り614GBでした。このうち96%がAPIリクエストによるもので、残りの4%が静的ファイルの配信です。

## 問題サーバー

CTFにおける問題サーバーは、参加者が攻撃する対象となるサービスを提供するサーバーです。以下の図に示すような単純な構成です。

{/* ![AWS上で構築されたサービスのアーキテクチャ図。AWSクラウド内には1つのリージョンが存在する。リージョン内には1つのAWS LambdaとVPCがある。VPC内には2つのアベイラビリティゾーンがあり、それぞれパブリックサブネット内に1つのAmazon EC2インスタンスが配置されている。ユーザーとAWS Lambdaは2つのEC2インスタンスに直接アクセスする。](./wanictf2024/architecture-chal-traffic.webp 'w=large') */}

ロードバランサは使用せず、参加者がEC2インスタンスに直接アクセスする構成としています。それぞれのIPアドレスに対応するDNSレコードを設定しており、参加者はいずれか1つのインスタンスを選んでアクセスします。

また、AWS Lambdaから各問題のヘルスチェックを行っています。ヘルスチェック結果はスコアサーバーやDiscordに送信しており、ユーザーや運営が不具合発生を把握できるようにしています。

## 踏み台サーバー

運営がVPC内のサーバーに容易にアクセスするための踏み台サーバーも用意しました。ECS上でTailscaleのクライアントを起動し、[subnet router](https://tailscale.com/kb/1019/subnets)として動作させています。

```terraform
resource "aws_ecs_task_definition" "bastion" {
  family = "bastion"

  cpu    = 256
  memory = 512

  execution_role_arn       = aws_iam_role.bastion_ecs_task_execution.arn
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  task_role_arn            = aws_iam_role.bastion_ecs_task.arn

  container_definitions = jsonencode([
    {
      name  = "tailscale"
      image = "ghcr.io/tailscale/tailscale:v1.66.3"
      environment = [
        { name = "TS_ROUTES", value = aws_vpc.main.cidr_block },
      ]
      secrets = [
        { name = "TS_AUTHKEY", valueFrom = var.tailscale_authkey_param_arn },
      ]
      linuxParameters = {
        initProcessEnabled = true
      }
    },
  ])
}
```
